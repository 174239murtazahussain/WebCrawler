# WebCrawler
The web crawler takes seed urls and from seed urls it downloads their html pages and then parse them and find other urls from it and filter them and does the process until it fetches 1000 urls.Following image shows that the yellow parts are perfromed in this project.
![web crawler](https://user-images.githubusercontent.com/43024106/120929338-246e4300-c702-11eb-8d3b-587c55966ef6.PNG)
